# ğŸ“° News Aggregator â€“ Sistema de Scraping y GestiÃ³n de Noticias

Sistema completo para **scraping, gestiÃ³n y anÃ¡lisis de noticias peruanas**, con autenticaciÃ³n de usuarios, planes de servicio, anÃ¡lisis con IA y un panel administrativo completo.

---

## ğŸ“‹ DescripciÃ³n General

Este proyecto integra:

- Scraping automatizado de mÃºltiples portales de noticias.
- API backend con FastAPI + MySQL.
- Frontend moderno construido con React + TypeScript.
- Modo Usuario y Modo Administrador.
- Planes Free y Plus.
- ChatBot con IA (Gemini AI).
- ExportaciÃ³n a Excel y filtros avanzados.

Ideal para aplicaciones de monitoreo de noticias, anÃ¡lisis informativo y automatizaciÃ³n de recolecciÃ³n de datos.

---

## ğŸš€ CaracterÃ­sticas Principales

### ğŸ”§ Funciones TÃ©cnicas
- âœ… Scraping automÃ¡tico y manual de noticias peruanas  
- âœ… AutenticaciÃ³n JWT con roles (Admin / Usuario)  
- âœ… Sistema de planes (Free y Plus)  
- âœ… AnÃ¡lisis de noticias con Google Gemini AI  
- âœ… ChatBot inteligente integrado  
- âœ… ExportaciÃ³n a Excel con filtros  
- âœ… API RESTful documentada (FastAPI Docs)  
- âœ… Panel administrativo completo  
- âœ… Frontend responsivo y moderno  

---

# ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

## ğŸ“Œ Prerrequisitos

AsegÃºrate de tener instalado:

- **Python 3.10+**
- **Node.js 16+**
- **MySQL 8.0+**
- **Git**

---

## ğŸ“¥ 1. Clonar el Repositorio

```bash
git clone https://github.com/maykolaracayo22/scraping_noticias.git
cd scraping_noticias
ğŸ—„ï¸ 2. ConfiguraciÃ³n de la Base de Datos (MySQL)
Crear base de datos manualmente:
sql
Copiar cÃ³digo
CREATE DATABASE news_aggregator_reddit;
O usar el script automÃ¡tico:
bash
Copiar cÃ³digo
cd backend_noticias
python create_database.py
ğŸ”§ 3. ConfiguraciÃ³n del Backend (FastAPI)
Navegar al backend:
bash
Copiar cÃ³digo
cd backend_noticias
Instalar dependencias:
bash
Copiar cÃ³digo
pip install -r requirements.txt
Crear archivo .env:
env
Copiar cÃ³digo
# Base de Datos MySQL
DB_HOST=localhost
DB_PORT=3306
DB_NAME=news_aggregator_reddit
DB_USER=tu_usuario_mysql
DB_PASSWORD=tu_password_mysql

# Google AI (Gemini)
GOOGLE_AI_API_KEY=TU_API_KEY_AQUI

# JWT
SECRET_KEY=tu_clave_secreta_muy_segura
ALGORITHM=HS256
Ejecutar el backend
bash
Copiar cÃ³digo
python main.py
Backend disponible en:
ğŸ‘‰ http://localhost:8000
ğŸ‘‰ DocumentaciÃ³n API: http://localhost:8000/docs

ğŸ¨ 4. ConfiguraciÃ³n del Frontend (React + TypeScript)
Navegar al frontend:
bash
Copiar cÃ³digo
cd frontend_noticias
Instalar dependencias:
bash
Copiar cÃ³digo
npm install
Ejecutar frontend:
bash
Copiar cÃ³digo
npm run dev
Frontend disponible en:
ğŸ‘‰ http://localhost:5173

ğŸ‘¤ 5. Credenciales de Acceso
Administrador (por defecto):
makefile
Copiar cÃ³digo
Email: admin@newsperu.com  
Password: 123456
Usuarios Free:
Se registran desde el sistema

Obtienen automÃ¡ticamente el plan Free

Pueden actualizar a Plus mediante Yape

ğŸ”‘ 6. ConfiguraciÃ³n Opcional â€“ Google Gemini AI
Para activar el anÃ¡lisis de noticias con IA:

Obtener tu API Key desde Google AI Studio

AÃ±adirla en el archivo .env:

env
Copiar cÃ³digo
GOOGLE_AI_API_KEY=TU_API_KEY_AQUI
ğŸ“ Estructura del Proyecto
bash
Copiar cÃ³digo
scraping_noticias/
â”œâ”€â”€ backend_noticias/          # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ models.py          # Modelos de la BD
â”‚   â”‚   â”œâ”€â”€ main.py            # AplicaciÃ³n principal
â”‚   â”‚   â”œâ”€â”€ crud.py            # Operaciones de BD
â”‚   â”‚   â””â”€â”€ scraper.py         # LÃ³gica de scraping
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ frontend_noticias/         # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ api/
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
ğŸ¯ Funcionalidades por Plan
ğŸ†“ Plan Free
Lectura de todas las noticias

Scraping bÃ¡sico

BÃºsqueda y filtros

Reportar noticias

Registro con correo

â­ Plan Plus (S/ 19.90 mensual)
Incluye todo lo del Free y ademÃ¡s:

AnÃ¡lisis de noticias con IA

ChatBot inteligente

ExportaciÃ³n avanzada a Excel

Scraping avanzado

MÃ¡s velocidad y mÃ¡s fuentes

ğŸ› SoluciÃ³n de Problemas Comunes
âŒ Error de conexiÃ³n a MySQL
Verifica que MySQL estÃ¡ corriendo

Confirma usuario y contraseÃ±a en .env

Asegura que la BD news_aggregator_reddit existe

âŒ Puerto en uso
Cambiar puerto backend (main.py):

python
Copiar cÃ³digo
uvicorn.run(app, host="0.0.0.0", port=8001)
Cambiar puerto frontend (vite.config.ts):

ts
Copiar cÃ³digo
server: { port: 5174 }
âŒ Error de dependencias
bash
Copiar cÃ³digo
pip install -r requirements.txt --force-reinstall

rm -rf node_modules package-lock.json
npm install
ğŸš€ Comandos RÃ¡pidos de Despliegue
Terminal 1 â€” Backend
bash
Copiar cÃ³digo
cd backend_noticias
python main.py
Terminal 2 â€” Frontend
bash
Copiar cÃ³digo
cd frontend_noticias
npm run dev
Sistema activo en:
ğŸ‘‰ http://localhost:5173 ğŸ‰

ğŸ“ Soporte
Si encuentras problemas:

Verifica prerrequisitos

Confirma que backend y frontend estÃ¡n activos

Revisa el archivo .env

Revisa logs de consola

ğŸ Estado del Proyecto
âœ” Proyecto funcional
âœ” Scraping operativo
âœ” Sistema de usuarios completo
âœ” IA integrada
âœ” Listo para producciÃ³n y demostraciones
